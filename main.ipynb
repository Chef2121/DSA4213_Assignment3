{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d9da8c2",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35764136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c93e8",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f48f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "dataset = load_dataset(\"arrmlet/political-social-x-us-sentiment-v1\")\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset['train'])} examples\")\n",
    "\n",
    "# Sample a smaller subset for training \n",
    "sample_size = 50000 \n",
    "sampled_dataset = dataset['train'].shuffle(seed=42).select(range(sample_size))\n",
    "print(f\"Sampled dataset size: {len(sampled_dataset)}\")\n",
    "\n",
    "# Convert sentiment scores to binary labels\n",
    "def process_batch(examples):\n",
    "    processed = {key: [] for key in examples.keys()}\n",
    "    processed['labels'] = []\n",
    "    \n",
    "    for i in range(len(examples['sentiment_negative'])):\n",
    "        neg_score = examples['sentiment_negative'][i]\n",
    "        pos_score = examples['sentiment_positive'][i]\n",
    "        neutral_score = examples['sentiment_neutral'][i]\n",
    "        \n",
    "        # Skip neutral-dominant examples\n",
    "        if neutral_score > max(neg_score, pos_score):\n",
    "            continue\n",
    "        \n",
    "        # Keep this example\n",
    "        for key in examples.keys():\n",
    "            processed[key].append(examples[key][i])\n",
    "        \n",
    "        # Add binary label\n",
    "        if pos_score > neg_score:\n",
    "            processed['labels'].append(1)  # Positive\n",
    "        else:\n",
    "            processed['labels'].append(0)  # Negative\n",
    "    \n",
    "    return processed\n",
    "\n",
    "# Apply processing\n",
    "processed_dataset = sampled_dataset.map(process_batch, batched=True, remove_columns=sampled_dataset.column_names)\n",
    "\n",
    "\n",
    "# Check label distribution\n",
    "label_counts = Counter(processed_dataset['labels'])\n",
    "\n",
    "# train/validation/test splits, 70,15,15\n",
    "train_test = processed_dataset.train_test_split(test_size=0.3, seed=23)\n",
    "val_test = train_test['test'].train_test_split(test_size=0.5, seed=23)\n",
    "\n",
    "train_dataset_raw = train_test['train']      \n",
    "eval_dataset_raw = val_test['train']        \n",
    "test_dataset_raw = val_test['test']          \n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"]\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495a9dc",
   "metadata": {},
   "source": [
    "# Tokenization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fed232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=False)\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = train_dataset_raw.map(tokenize_function, batched=True)\n",
    "eval_dataset = eval_dataset_raw.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset_raw.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove unnecessary columns (keep only what we need for training)\n",
    "columns_to_keep = ['input_ids', 'attention_mask', 'labels']\n",
    "columns_to_remove = [col for col in train_dataset.column_names if col not in columns_to_keep]\n",
    "\n",
    "print(f\"Removing columns: {columns_to_remove}\")\n",
    "train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
    "eval_dataset = eval_dataset.remove_columns(columns_to_remove)\n",
    "test_dataset = test_dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "print(\"Final columns:\", train_dataset.column_names)\n",
    "print(\"Sample labels:\", train_dataset['labels'][:10])\n",
    "\n",
    "# Final label distribution check\n",
    "train_label_counts = Counter(train_dataset['labels'])\n",
    "eval_label_counts = Counter(eval_dataset['labels'])\n",
    "test_label_counts = Counter(test_dataset['labels'])\n",
    "\n",
    "print(f\"\\nFinal label distributions:\")\n",
    "print(f\"Train: {train_label_counts}\")\n",
    "print(f\"Validation: {eval_label_counts}\")\n",
    "print(f\"Test: {test_label_counts}\")\n",
    "\n",
    "# Define compute metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61960db8",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lora_simple(train_dataset, eval_dataset, hyperparams, trial_name):\n",
    "    \n",
    "    print(f\"TRIAL: {trial_name}\")\n",
    "    print(f\"Parameters: {hyperparams}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    tokenizer_trial = AutoTokenizer.from_pretrained(model_name)\n",
    "    model_trial = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    \n",
    "    if tokenizer_trial.pad_token is None:\n",
    "        tokenizer_trial.pad_token = tokenizer_trial.eos_token\n",
    "    \n",
    "    # LoRA config\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        r=hyperparams['r'],\n",
    "        lora_alpha=hyperparams['alpha'],\n",
    "        lora_dropout=hyperparams['dropout'],\n",
    "        target_modules=[\"q_lin\", \"v_lin\"]\n",
    "    )\n",
    "    \n",
    "    model_trial = get_peft_model(model_trial, lora_config)\n",
    "    \n",
    "    # Training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./temp_trial_{trial_name}\",\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=hyperparams['batch_size'],\n",
    "        per_device_eval_batch_size=32,\n",
    "        learning_rate=hyperparams['lr'],\n",
    "        warmup_steps=200,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=300,\n",
    "        save_strategy=\"no\",  \n",
    "        load_best_model_at_end=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=None,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model_trial,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer_trial),\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"F1: {eval_results['eval_f1']:.4f}, Time: {training_time:.1f}s\")\n",
    "        \n",
    "        return {\n",
    "            'trial': trial_name,\n",
    "            'hyperparams': hyperparams,\n",
    "            'f1': eval_results['eval_f1'],\n",
    "            'accuracy': eval_results['eval_accuracy'],\n",
    "            'time': training_time,\n",
    "            'model': model_trial,\n",
    "            'tokenizer': tokenizer_trial\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define common hyperparameter combinations to test\n",
    "hyperparameter_trials = [\n",
    "    {'r': 16, 'alpha': 32, 'dropout': 0.1, 'lr': 2e-5, 'batch_size': 16},\n",
    "    {'r': 8, 'alpha': 16, 'dropout': 0.1, 'lr': 2e-5, 'batch_size': 16},\n",
    "    {'r': 32, 'alpha': 64, 'dropout': 0.1, 'lr': 2e-5, 'batch_size': 16},\n",
    "    {'r': 16, 'alpha': 32, 'dropout': 0.1, 'lr': 1e-5, 'batch_size': 16},\n",
    "    {'r': 16, 'alpha': 32, 'dropout': 0.1, 'lr': 5e-5, 'batch_size': 16},\n",
    "    {'r': 16, 'alpha': 32, 'dropout': 0.2, 'lr': 2e-5, 'batch_size': 16},\n",
    "    {'r': 16, 'alpha': 32, 'dropout': 0.1, 'lr': 2e-5, 'batch_size': 8},\n",
    "    {'r': 8, 'alpha': 16, 'dropout': 0.1, 'lr': 5e-5, 'batch_size': 16},\n",
    "]\n",
    "\n",
    "trial_results = []\n",
    "best_f1 = 0\n",
    "best_trial = None\n",
    "\n",
    "for i, params in enumerate(hyperparameter_trials, 1):\n",
    "    trial_name = f\"trial_{i}\"\n",
    "    \n",
    "    result = train_lora_simple(train_dataset, eval_dataset, params, trial_name)\n",
    "    \n",
    "    if result:\n",
    "        trial_results.append(result)\n",
    "        \n",
    "\n",
    "\n",
    "if trial_results:\n",
    "    \n",
    "    trial_results.sort(key=lambda x: x['f1'], reverse=True)\n",
    "    \n",
    "    for i, result in enumerate(trial_results, 1):\n",
    "        params = result['hyperparams']\n",
    "        print(f\"{i}. F1: {result['f1']:.4f} | r={params['r']}, Œ±={params['alpha']}, lr={params['lr']:.0e}, batch={params['batch_size']}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0351237",
   "metadata": {},
   "source": [
    "## Fine tune with LoRA with optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfcba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "final_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "final_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "if final_tokenizer.pad_token is None:\n",
    "    final_tokenizer.pad_token = final_tokenizer.eos_token\n",
    "\n",
    "final_lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"]\n",
    ")\n",
    "\n",
    "final_model = get_peft_model(final_model, final_lora_config)\n",
    "\n",
    "# Training arguments\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir=\"./final_lora_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=300,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    dataloader_pin_memory=False,\n",
    "    report_to=None,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "# Train final model\n",
    "final_trainer = Trainer(\n",
    "    model=final_model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=final_tokenizer),\n",
    ")\n",
    "\n",
    "final_trainer.train()\n",
    "\n",
    "final_model.save_pretrained(\"./final_lora_model\")\n",
    "final_tokenizer.save_pretrained(\"./final_lora_model\")\n",
    "\n",
    "model = final_model\n",
    "tokenizer = final_tokenizer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d495e7b2",
   "metadata": {},
   "source": [
    "## Full fine-tuning \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1fb826",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer_full = AutoTokenizer.from_pretrained(model_name)\n",
    "model_full = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Fix padding token to ensure equal length sequences by end of seq\n",
    "if tokenizer_full.pad_token is None:\n",
    "    tokenizer_full.pad_token = tokenizer_full.eos_token\n",
    "\n",
    "print(f\"Full model parameters: {sum(p.numel() for p in model_full.parameters()):,}\")\n",
    "\n",
    "data_collator_full = DataCollatorWithPadding(tokenizer=tokenizer_full)\n",
    "\n",
    "# Define compute metrics function for full fine-tuning\n",
    "def compute_metrics_full(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "\n",
    "training_args_full = TrainingArguments(\n",
    "    output_dir=\"./output_full_finetune_political\",\n",
    "    num_train_epochs=3,  \n",
    "    per_device_train_batch_size=8,  \n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,  \n",
    "    warmup_steps=300,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs_full_political\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_accumulation_steps=2,  \n",
    "    save_total_limit=2,\n",
    "    report_to=None,\n",
    ")\n",
    "\n",
    "\n",
    "trainer_full = Trainer(\n",
    "    model=model_full,\n",
    "    args=training_args_full,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics_full,\n",
    "    data_collator=data_collator_full,\n",
    ")\n",
    "\n",
    "\n",
    "trainer_full.train()\n",
    "\n",
    "\n",
    "model_full.save_pretrained(\"./full_finetune_political_model\")\n",
    "tokenizer_full.save_pretrained(\"./full_finetune_political_model\")\n",
    "print(\"Full fine-tuning completed and model saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334554b9",
   "metadata": {},
   "source": [
    "## Comparison plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5be4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_data = {\n",
    "    'step': [500, 1000, 1500, 2000, 2500, 3000],\n",
    "    'training_loss': [0.349900, 0.169400, 0.154800, 0.142300, 0.133400, 0.135500],\n",
    "    'validation_loss': [0.161679, 0.143583, 0.143701, 0.145310, 0.138031, 0.138588],\n",
    "    'accuracy': [0.932829, 0.945624, 0.943879, 0.942425, 0.950567, 0.948822],\n",
    "    'f1': [0.933017, 0.945274, 0.944018, 0.942884, 0.950432, 0.948718]\n",
    "}\n",
    "\n",
    "full_data = {\n",
    "    'step': [500, 1000, 1500, 2000, 2500, 3000],\n",
    "    'training_loss': [0.191400, 0.166200, 0.105700, 0.100600, 0.026700, 0.041700],\n",
    "    'validation_loss': [0.161104, 0.143089, 0.178702, 0.180091, 0.211020, 0.203565],\n",
    "    'accuracy': [0.944461, 0.947659, 0.946787, 0.949113, 0.955801, 0.955220],\n",
    "    'f1': [0.944144, 0.948093, 0.947299, 0.949618, 0.955525, 0.955144]\n",
    "}\n",
    "\n",
    "lora_df = pd.DataFrame(lora_data)\n",
    "full_df = pd.DataFrame(full_data)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('LoRA vs Full Fine-tuning: Complete Training Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1Training Loss Comparison\n",
    "axes[0, 0].plot(lora_df['step'], lora_df['training_loss'], 'b-o', label='LoRA Training', linewidth=2.5, markersize=6)\n",
    "axes[0, 0].plot(full_df['step'], full_df['training_loss'], 'r-s', label='Full FT Training', linewidth=2.5, markersize=6)\n",
    "axes[0, 0].set_xlabel('Training Steps')\n",
    "axes[0, 0].set_ylabel('Training Loss')\n",
    "axes[0, 0].set_title('Training Loss Comparison')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2 Validation Loss Comparison (Main Focus)\n",
    "axes[0, 1].plot(lora_df['step'], lora_df['validation_loss'], 'b--o', label='LoRA Validation', linewidth=3, markersize=8)\n",
    "axes[0, 1].plot(full_df['step'], full_df['validation_loss'], 'r--s', label='Full FT Validation', linewidth=3, markersize=8)\n",
    "axes[0, 1].set_xlabel('Training Steps')\n",
    "axes[0, 1].set_ylabel('Validation Loss')\n",
    "axes[0, 1].set_title('Validation Loss Comparison', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3 Combined Training & Validation\n",
    "axes[0, 2].plot(lora_df['step'], lora_df['training_loss'], 'b-', label='LoRA Training', linewidth=2)\n",
    "axes[0, 2].plot(lora_df['step'], lora_df['validation_loss'], 'b--', label='LoRA Validation', linewidth=2, alpha=0.8)\n",
    "axes[0, 2].plot(full_df['step'], full_df['training_loss'], 'r-', label='Full FT Training', linewidth=2)\n",
    "axes[0, 2].plot(full_df['step'], full_df['validation_loss'], 'r--', label='Full FT Validation', linewidth=2, alpha=0.8)\n",
    "axes[0, 2].set_xlabel('Training Steps')\n",
    "axes[0, 2].set_ylabel('Loss')\n",
    "axes[0, 2].set_title('Training vs Validation Loss')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4 Accuracy Comparison\n",
    "axes[1, 0].plot(lora_df['step'], lora_df['accuracy'], 'b-o', label='LoRA Accuracy', linewidth=2.5, markersize=6)\n",
    "axes[1, 0].plot(full_df['step'], full_df['accuracy'], 'r-s', label='Full FT Accuracy', linewidth=2.5, markersize=6)\n",
    "axes[1, 0].set_xlabel('Training Steps')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].set_title('Accuracy Progression')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5 F1 Score Comparison\n",
    "axes[1, 1].plot(lora_df['step'], lora_df['f1'], 'b-o', label='LoRA F1', linewidth=2.5, markersize=6)\n",
    "axes[1, 1].plot(full_df['step'], full_df['f1'], 'r-s', label='Full FT F1', linewidth=2.5, markersize=6)\n",
    "axes[1, 1].set_xlabel('Training Steps')\n",
    "axes[1, 1].set_ylabel('F1 Score')\n",
    "axes[1, 1].set_title('F1 Score Progression')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6 Overfitting Analysis (Train-Val Gap)\n",
    "lora_gap = lora_df['training_loss'] - lora_df['validation_loss']\n",
    "full_gap = full_df['training_loss'] - full_df['validation_loss']\n",
    "\n",
    "axes[1, 2].plot(lora_df['step'], lora_gap, 'b-o', label='LoRA Gap', linewidth=2.5, markersize=6)\n",
    "axes[1, 2].plot(full_df['step'], full_gap, 'r-s', label='Full FT Gap', linewidth=2.5, markersize=6)\n",
    "axes[1, 2].axhline(y=0, color='gray', linestyle=':', alpha=0.7)\n",
    "axes[1, 2].set_xlabel('Training Steps')\n",
    "axes[1, 2].set_ylabel('Training - Validation Loss')\n",
    "axes[1, 2].set_title('Overfitting Analysis (Gap)')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1814cec",
   "metadata": {},
   "source": [
    "## Prediction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33300d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "lora_base_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "lora_model = PeftModel.from_pretrained(lora_base_model, \"./final_lora_model\")\n",
    "lora_tokenizer = AutoTokenizer.from_pretrained(\"./final_lora_model\")\n",
    "\n",
    "full_model = AutoModelForSequenceClassification.from_pretrained(\"./full_finetune_political_model\")\n",
    "full_tokenizer = AutoTokenizer.from_pretrained(\"./full_finetune_political_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61f51bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if lora_tokenizer.pad_token is None:\n",
    "    lora_tokenizer.pad_token = lora_tokenizer.eos_token\n",
    "if full_tokenizer.pad_token is None:\n",
    "    full_tokenizer.pad_token = full_tokenizer.eos_token\n",
    "\n",
    "\n",
    "def predict_with_lora(text):\n",
    "    inputs = lora_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = lora_model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "    confidence = predictions[0][predicted_class].item()\n",
    "    \n",
    "    label_map = {0: \"negative\", 1: \"positive\"}\n",
    "    predicted_label = label_map[predicted_class]\n",
    "    \n",
    "    return predicted_label, confidence\n",
    "\n",
    "def predict_with_full(text):\n",
    "    inputs = full_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = full_model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "    confidence = predictions[0][predicted_class].item()\n",
    "    \n",
    "    label_map = {0: \"negative\", 1: \"positive\"}\n",
    "    predicted_label = label_map[predicted_class]\n",
    "    \n",
    "    return predicted_label, confidence\n",
    "\n",
    "def predict_with_base_model(text):\n",
    "    base_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    base_model_clean = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "    \n",
    "    if base_tokenizer.pad_token is None:\n",
    "        base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "    \n",
    "    inputs = base_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = base_model_clean(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "    confidence = predictions[0][predicted_class].item()\n",
    "    \n",
    "    label_map = {0: \"negative\", 1: \"positive\"}\n",
    "    predicted_label = label_map[predicted_class]\n",
    "    \n",
    "    return predicted_label, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88933c48",
   "metadata": {},
   "source": [
    "## Evaluation against test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659fb47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_test_set(model, tokenizer, test_dataset, model_name):\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    confidences = []\n",
    "    \n",
    "    # Predict on test set\n",
    "    for i, example in enumerate(test_dataset):\n",
    "        text = tokenizer.decode(example['input_ids'], skip_special_tokens=True)\n",
    "        true_label = example['labels']\n",
    "        \n",
    "        # Get prediction\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "        confidence = probs[0][predicted_class].item()\n",
    "        \n",
    "        predictions.append(predicted_class)\n",
    "        true_labels.append(true_label)\n",
    "        confidences.append(confidence)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
    "\n",
    "    \n",
    "    # Detailed results\n",
    "    print(f\"{model_name} TEST RESULTS:\")\n",
    "    print(f\"Accuracy:           {accuracy:.4f}\")\n",
    "    print(f\"F1 Score (weighted): {f1:.4f}\")\n",
    "    print(f\"Precision (weighted): {precision:.4f}\")\n",
    "    print(f\"Recall (weighted):    {recall:.4f}\")\n",
    "    print(f\"Average Confidence:   {np.mean(confidences):.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_weighted': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels,\n",
    "        'confidences': confidences,\n",
    "    }\n",
    "\n",
    "base_model_eval = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "base_tokenizer_eval = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "if base_tokenizer_eval.pad_token is None:\n",
    "    base_tokenizer_eval.pad_token = base_tokenizer_eval.eos_token\n",
    "\n",
    "\n",
    "base_results = evaluate_model_on_test_set(base_model_eval, base_tokenizer_eval, test_dataset, \"Base Model\")\n",
    "\n",
    "lora_results = evaluate_model_on_test_set(lora_model, lora_tokenizer, test_dataset, \"LoRA\")\n",
    "\n",
    "full_results = evaluate_model_on_test_set(full_model, full_tokenizer, test_dataset, \"Full Fine-tuned\")\n",
    "\n",
    "comparison_metrics = ['accuracy', 'f1_weighted', 'precision', 'recall']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e206a90a",
   "metadata": {},
   "source": [
    "## test against sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec26ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [\n",
    "    \"I am really struggling to support the current president after his scandal\", #neg\n",
    "    \"I love our Mayor, he has really cleaned up the streets\", #pos\n",
    "    \"Damn, why on earth did income tax rise again\", #neg\n",
    "    \"This year our economy has done a 180 and employment rates are up again\", #pos\n",
    "    \"Trust the fool at the top to mess up the diplomatic relationships\", #neg\n",
    "    \"I have high hopes for President John\", #pos\n",
    "    \"If I were Prime Minister our economy would not be down\", #neg\n",
    "    \"The tax relief really came in handy, i am strapped for cash\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cacaf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testpred(test_texts):\n",
    "\n",
    "    lora_avg = 0\n",
    "    full_avg = 0\n",
    "    base_avg = 0\n",
    "    ntext = len(test_texts)\n",
    "\n",
    "    for i, text in enumerate(test_texts, 1):\n",
    "        lora_pred, lora_conf = predict_with_lora(text)\n",
    "        full_pred, full_conf = predict_with_full(text)\n",
    "        base_pred, base_conf = predict_with_base_model(text)\n",
    "        \n",
    "        lora_avg += lora_conf\n",
    "        full_avg += full_conf\n",
    "        base_avg += base_conf\n",
    "        \n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"LoRA Model:      {lora_pred} (conf: {lora_conf})\")\n",
    "        print(f\"Full Fine-tuned: {full_pred} (conf: {full_conf})\")\n",
    "        print(f\"Base Model:      {base_pred} (conf: {base_conf})\")\n",
    "\n",
    "    print(f\"LoRA average confidence: {lora_avg/ntext}\")\n",
    "    print(f\"Full average confidence: {full_avg/ntext}\")\n",
    "    print(f\"Base average confidence: {base_avg/ntext}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32647e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "testpred(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f58be2",
   "metadata": {},
   "source": [
    "## Sarcasm evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88870145",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_texts = [\n",
    "    \"Oh sure, raising taxes will definitely make everyone rich overnight,brilliant plan!\", \n",
    "    \"Because nothing says progress like arguing about the same policy for the tenth year in a row.\", \n",
    "    \"Great idea to cut education funding, who needs smart voters anyway?\",\n",
    "    \"Of course, banning debates will totally improve democracy. Genius move!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4505886",
   "metadata": {},
   "outputs": [],
   "source": [
    "testpred(s_texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
